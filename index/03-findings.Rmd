# Findings {#findings .unnumbered}

Should be organized as follows:

 - Results of descriptive analyses
 - Modeling results
 - Results of model performance and validation

## Results of descriptive analyses  {#findings-descriptive .unnumbered}

###Distribution of Quantity

Order quantity is typically less than 50,000, with a few orders significantly higher. Specifically, most order quantity are less than 30,000. See \@ref(fig:Quantity_Histogram) below for histogram on the quantity distribution.

```{r Quantity_Histogram, echo=FALSE, warning=FALSE,  message=FALSE, fig.cap="Histogram of Quantity", fig.width=6}
folder <- "C:/Users/Bob/Desktop/Capstone"
rawdata <- read.csv(paste(folder,"Sales_data_for_Capstone_Project_-_Student_Version_-_v3_CLEAN.csv",sep = "/"))
coerce_cols <- c("fiscalquartername","FiscalPeriod","Company","Top_Most_BP","BusinessPartner","Item_Number","Item_Description","Ship_to_State","Ship_to_City","Line_of_Business")

rawdata[coerce_cols] <- lapply(rawdata[coerce_cols],as.factor)

rawdata$Planned_Delivery_Date <- as.Date(rawdata$Planned_Delivery_Date)
cols_of_interest <- c("Company","Top_Most_BP","Item_Number","Ship_to_State","Quantity","Planned_Delivery_Date","Line_of_Business")
df <- rawdata[cols_of_interest]
levels(df$Line_of_Business) <- c("CITRUS","PFOTHR","TOMATO","TROPCL")

#Filter to only Tomato
df <- df[df$Line_of_Business=="TOMATO",]
library(ggplot2)
#Quantity Distribution
ggplot(df,aes(x=Quantity))+
  geom_histogram()+
  scale_x_continuous(labels = scales::comma)
```

###Distribution of Quantity over time

The figure below breaks down shipments over time. There is a large degree of seasonal behavior. Tomato bag sales increase during summer time from June to August and begin to drop off in September and October. Quantity is very small during other months of the year.

```{r Quantity_Time, echo=FALSE, warning=FALSE,  message=FALSE, fig.cap="Quantity over Time", fig.width=6}
library(lubridate)
library(dplyr)
library(stringr)
library(tidyr)
#Quantity vs. Planned Delivery Date
proc_data <- df %>%
  #mutate produces month & year columns
  mutate(month = format(Planned_Delivery_Date, "%m"), year = format(Planned_Delivery_Date, "%Y")) %>%
  #groupby gets me each of the data elements
  group_by(month, year, Item_Number) %>%
  #summarise gets me the sum of quantity
  summarise(tot = sum(Quantity)) 

#generate time series complete data
proc_data_complete <- proc_data%>%
  #add in a column for the first of the month to be processed as date data
  mutate(dt = paste(year,month,"01",sep="-")) %>%
  #add a date/time version of Date
  mutate(Date=as.Date(dt))%>%
  #complete the 
  complete(Item_Number,Date = seq.Date(min(Date), max(Date), by="month")) %>%
  #modify the total column to replace NA's
  mutate(tot = if_else(is.na(tot),0,tot))
proc_data_complete%>%
  group_by(Date)%>%
  summarise(tot = sum(tot))%>%
  ggplot(aes(x=Date, y=tot))+
  geom_line()+
  scale_y_continuous(labels = scales::comma)+
  ylab("Aggregate Bag Quantity")+
  xlab("Planned Delivery Date")+
  ggtitle("Aggregate Bag Quantity Vs. Planned Delivery Date")
```

###Distribution of Quantity over Time

The figure below breaks down shipments over time. There is a large degree of seasonal behavior. Tomato bag sales increase during summer time from June to August and begin to drop off in September and October. Quantity is very small during other months of the year.

```{r Quantity_Month, echo=FALSE, warning=FALSE,  message=FALSE, fig.cap="Monthy Quantity by Year", fig.width=6}
proc_data_complete %>%
  group_by(Date)%>%
  ggplot(aes(x = year, y = tot))+ 
  geom_col() + 
  scale_y_continuous(labels=scales::comma) +
  facet_wrap(~ month)+
  scale_x_discrete(labels =c("2010","","","","2014","","","",""))+
  ylab("Aggregate Bag Quantity")+
  xlab("Planned Delivery Date")
```

###Quantity by Item Number

In the figure below, total order quantity are aggregated by item number and then ordered by descending total quantity. Over 86% of bag quantities are driven by the top 10 out of a total of 60 item numbers.
```{r IN_cumulative, echo=FALSE, warning=FALSE,  message=FALSE, fig.cap="Cumulative Sales by Item Number", fig.width=6}
#Cumulative aggregate by Item Number
totals_by_Item_Number <- aggregate(Quantity~Item_Number,data=df,sum)
totals_by_Item_Number <- totals_by_Item_Number[order(totals_by_Item_Number$Quantity,decreasing = T),]

tot_IN_cum_sum <- cumsum(totals_by_Item_Number$Quantity)
cum_tot_pct <- tot_IN_cum_sum / sum(totals_by_Item_Number$Quantity)
cum_tot_pct <- as.data.frame(cum_tot_pct)
cum_tot_pct$ID <- seq(1,length(cum_tot_pct$cum_tot_pct), by=1)
ggplot(data=cum_tot_pct,aes(x=ID,y=cum_tot_pct))+
  geom_point()+
  labs(x="Item Number Index",y="Percent Total",title="Cumulative Aggregate Quantity By Item Number Index") +
  geom_vline(xintercept = 10)+
  geom_text(aes(x=20,y=0.86),label=paste("Top Ten:",prettyNum(cum_tot_pct[10,]$cum_tot_pct*100,big.mark = ",",format="d",digits=2),"%"))+
  scale_y_continuous(labels = scales::percent)
```

###Quantity by Top Business Partner

The figure below aggregates quantity by item number and then orders quantity descendingly. It suggests that over 93% of bag quantities are driven by the top 10 out of the 23 top business partners. 

```{r BP_Cumulative, echo=FALSE, warning=FALSE,  message=FALSE, fig.cap="Cumulative Sales by Business Partner", fig.width=6}
totals_by_BP <- aggregate(Quantity~Top_Most_BP,data=df,sum)
totals_by_BP <- totals_by_BP[order(totals_by_BP$Quantity,decreasing = T),]
tot_BP_cum_sum <- cumsum(totals_by_BP$Quantity)
cum_tot_pct_BP <- tot_BP_cum_sum / sum(totals_by_BP$Quantity)
cum_tot_pct_BP <- as.data.frame(cum_tot_pct_BP)
cum_tot_pct_BP$ID <- seq(1,length(cum_tot_pct_BP$cum_tot_pct_BP), by=1)
ggplot(data=cum_tot_pct_BP,aes(x=ID,y=cum_tot_pct_BP))+
  geom_point()+
  labs(x="Item Number Index",y="Percent Total",title="Cumulative Aggregate Quantity By Business Partner Index") +
  geom_vline(xintercept = 10)+
  geom_text(aes(x=13,y=0.93),label=paste("Top Ten:",prettyNum(cum_tot_pct_BP[10,]$cum_tot_pct*100,big.mark = ",",format="d",digits=2),"%"))+
  scale_y_continuous(labels = percent)
```

###External Data Collection

We collected Social Media data from two sources: Twitter and the Google Trends. Google Trends summarised US monthly search statistics from 2004 to 2019. The twitter data collected are of US users from 2007 to 2018.

Google Trends returns a single value showing how frequently a given search term (e.g. ‘Tomato’) goes in Google’s search engine relative to its total search frequency for a given period. 

Twitter data were gathered by collecting relevant tweets from Twitter using key search words  as shown in Table __ below. We can grab a tweet based on defined keyword from Twitter by calling the Twitter API function. Subsequently, we can categorize opinions expressed in a piece of text, in order to determine opinion on our research (i.e, positive, negative, or neutral). 

```{r External_Data}
library(kableExtra)
External_Data_Source <- c('Google Trend','Twitter')
Frequency_Granularity <- c('Monthly','Monthly')
Range <- c('2004-2019','2007-2019')
Size <- c('182','')
Data_Type <- c('Numeric','Text, Numeric')
test_table <- cbind(External_Data_Source,
                    Frequency_Granularity,
                    Range,
                    Size,
                    Data_Type)
test_table <- data.frame(test_table)
colnames(test_table) <- c('External Data Source','Frequency / Granularity','Range','Size','Data Type')

kable(test_table, digit = 7, align = "r", caption = "External Data Sources", 
      format = "latex", longtable = TRUE)
```

Table __ above shows the description of the social media dataset. The google trend dataset based on relative search frequency is on a monthly basis. The monthly dataset at the time of reporting has 182 rows.

The Twitter data are sampled via a quasi-random approach that grabs data monthly over the entire period of  2007 - 2018. We had to employ this method of querying due to the nature of the Twitter API and its restrictions on total tweets returned. The Twitter API returns tweets in reverse chronological order. The total number of tweets that would mention one of our keywords would be vastly larger than the number we could collect over the course of our data collection period (January 2019 - March 2019). Limited in this way, we decided to strategically collect tweets from each month of the year between January 2007 and March 2019.

Assumptions & Limitation
 
Google Trends
Assumptions
We limited the Google Trends search to the United States.
The keywords are independent of each other.

Limitation: The actual number of searches for the term being queried is not made available. Instead, the data are reported from 0-100, where 100 represents the maximum relative search frequency.

Resolution: Each term selected will be queried individually.
 
Twitter Datasets
Assumptions
We limited Twitter data to the United States.
The frequency of tweets we collected for each keyword will be independent of the time period in which we collected them.

Limitations 
The demographic information of the twitter account user cannot be determined.
With the limitation of our premium account API activity, we can only submit 100 requests to collect tweet per month. Per each request, we can get a maximum of 500 tweets back.
The language associated with the Twitter account returned from the API does not guarantee the language of the Tweet. While we specified English language tweets, we received many tweets that were not in English. We subsequently dropped these tweets.

Plan for use
 
Twitter data
Develop a sentiment index for all tweets using natural language processing techniques
Utilize SpaCy Library to analyze how similar or discrepant the meaning of tweets among each keyword

Google Trends
Correlation analysis of frequency of individual search terms compared to quantity
Subsequent seasonality & trend analysis to identify meaningful predictors

Twitter Data

We have collected tweets based on the keywords in Table 3 below. Tweets were aggregated on a monthly basis. Figure 20 below shows how frequently the keywords appears in the returned tweets over time. Pizza is more frequently mentioned in Twitter than other keywords.

```{r}
kable(test_table, digit = 7, align = "r", caption = "External Data Sources", 
      format = "latex", longtable = TRUE)
```


## Modeling results  {.unnumbered}

First, use a `t.test()` to test _if_ dosage leads to growth of incisor length. From the results below, it appears every test rejects the null hypothesis.



## Results of model performance and validation  {.unnumbered}

Next, subset the `ToothGrowth` data into seperate data sets defined by supplement dose of 0.5, 1, and 2 mg. This allow us to controlling for dose increases of _economic_ significance.

Subset tooth data into a separate `data.frame` for each dosage level. Then Execute the `t.test()` function for the dosage of 0.5 mg and display the results. 

```{r}

```



